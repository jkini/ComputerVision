{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as ocr\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import shutil\n",
    "from tqdm import tqdm,trange\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "from numpy.random import choice\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import keras as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation,BatchNormalization,Lambda, Input, Conv2D, Dense,MaxPooling2D, Dropout, Flatten, UpSampling2D, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNextbatch(batch_size=10, stepNumber=0, imgDim = 128, imageFiles = 'list/of/files', pathSplitIndex = 2, classValLength = 4):\n",
    "    import cv2 as ocr\n",
    "    try:\n",
    "        input_label_list = []\n",
    "        input_data_list = []\n",
    "        counter = 0\n",
    "        start = stepNumber\n",
    "        end = stepNumber + batch_size\n",
    "\n",
    "        for imgF in imageFiles[start:end]:\n",
    "          \n",
    "            input_label_list.append(int(imgF.split('/')[pathSplitIndex][0:classValLength]))\n",
    "            input_data_list.append(ocr.imread(imgF,0))\n",
    "\n",
    "        return (np.array(input_data_list).reshape(-1,imgDim,imgDim,1)\n",
    "                , np.array(input_label_list).astype('int64'))\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        \n",
    "def getData(dataPath='path/To/Data',channels = 0, pathSplitIndex = 2, classValLength = 4, classCount = None, classValue = None):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    imageClass = []\n",
    "    imageFiles = [] \n",
    "    input_label_list = []\n",
    "    input_data_list = []   \n",
    "    selectedFilePaths = []\n",
    "    imageFiles = [dataPath+'/'+f for f in listdir(dataPath) if isfile(join(dataPath, f))]\n",
    "    \n",
    "    for imgF in tqdm(imageFiles, total=len(imageFiles), unit=\"files\"): \n",
    "#         print(imgF)\n",
    "        c = int(imgF.split('/')[pathSplitIndex][0:classValLength])\n",
    "        if( classCount!=None and classValue == None):\n",
    "            imageClass.append(c) \n",
    "            if(len(Counter(imageClass).keys())>classCount):\n",
    "                imageClass[classCount] = imageClass[classCount-1]\n",
    "            else:\n",
    "#                 input_label_list.append(c)\n",
    "#                 input_data_list.append(ocr.imread(imgF,channels))\n",
    "                selectedFilePaths.append(imgF)\n",
    "        elif(classCount==None and classValue!=None):\n",
    "            if(c in classValue):\n",
    "#                 input_label_list.append(c)\n",
    "#                 input_data_list.append(ocr.imread(imgF,channels))\n",
    "                selectedFilePaths.append(imgF)\n",
    "        imageClass = list(set(imageClass))\n",
    "    return np.array(selectedFilePaths)\n",
    "\n",
    "def contrastive_loss(left_model, right_model, y, margin):\n",
    "    with tf.name_scope(\"contrastive-loss\"):\n",
    "        d = tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(left_model,right_model), 2), 1, keep_dims=True))\n",
    "        part1= y * tf.square(d)    \n",
    "        part2 = (1 - y) * tf.square(tf.maximum((margin - d),0))\n",
    "        return tf.reduce_mean(part1 + part2) /2\n",
    "    \n",
    "\n",
    "images_folder_tr = 'images-lfw/'\n",
    "images_folder_val = images_folder_tr\n",
    "def gen_random_train_batch(nb_examples):\n",
    "    out_l,out_r,out_y = [],[],[]\n",
    "    li = random.sample(range(len(df_train)), nb_examples)\n",
    "    for i in li:\n",
    "        im_l = df_train.iloc[i]['name1']+\"/\"+df_train.iloc[i]['name1']+'_'+str(df_train.iloc[i]['im1']).zfill(4)+\".jpg\"\n",
    "        im_r = df_train.iloc[i]['name2']+\"/\"+df_train.iloc[i]['name2']+'_'+str(df_train.iloc[i]['im2']).zfill(4)+\".jpg\"\n",
    "        \n",
    "        \n",
    "        out_l.append(ocr.imread(images_folder_tr+im_l,0))\n",
    "        out_r.append(ocr.imread(images_folder_tr+im_r,0))\n",
    "        out_y.append(df_train.iloc[i]['label'])\n",
    "    out_l = ((np.array(out_l))/255.0).astype('float32')\n",
    "    out_r = ((np.array(out_r))/255.0).astype('float32')\n",
    "    out_y = np.array(out_y).reshape((-1,1))\n",
    "    return out_l.reshape(-1,imDim,imDim,1), out_r.reshape(-1,imDim,imDim,1), np.array(out_y)\n",
    "\n",
    "def gen_random_val_batch(nb_examples):\n",
    "    out_l,out_r,out_y = [],[],[]\n",
    "    li = random.sample(range(len(df_val)), nb_examples)\n",
    "    for i in li:\n",
    "        \n",
    "        im_l = df_val.iloc[i]['name1']+\"/\"+df_val.iloc[i]['name1']+'_'+str(df_val.iloc[i]['im1']).zfill(4)+\".jpg\"\n",
    "        im_r = df_val.iloc[i]['name2']+\"/\"+df_val.iloc[i]['name2']+'_'+str(df_val.iloc[i]['im2']).zfill(4)+\".jpg\"\n",
    "        \n",
    "        \n",
    "        out_l.append(ocr.imread(images_folder_val+im_l,0))\n",
    "        out_r.append(ocr.imread(images_folder_val+im_r,0))\n",
    "        out_y.append(df_val.iloc[i]['label'])\n",
    "    out_l = ((np.array(out_l))/255.0).astype('float32')\n",
    "    out_r = ((np.array(out_r))/255.0).astype('float32')\n",
    "    out_y = np.array(out_y).reshape((-1,1))\n",
    "    return out_l.reshape(-1,imDim,imDim,1), out_r.reshape(-1,imDim,imDim,1), np.array(out_y)\n",
    "\n",
    "# def gen_random_train_batch_sift(nb_examples):\n",
    "#     out_l,out_r,sift_l,sift_r,out_y = [],[],[],[],[]\n",
    "#     li = random.sample(range(len(df_train)), nb_examples)\n",
    "#     for i in li:\n",
    "#         out_l.append(ocr.imread(images_folder_tr+df_train.iloc[i]['left'],0))\n",
    "#         out_r.append(ocr.imread(images_folder_tr+df_train.iloc[i]['right'],0))\n",
    "#         sift_l.append(df_train.iloc[i][3:259])\n",
    "#         sift_r.append(df_train.iloc[i][259:])\n",
    "#         out_y.append(df_train.iloc[i]['label'])\n",
    "#     out_l = ((255.01 - np.array(out_l))/255.0).astype('float32')\n",
    "#     out_r = ((255.01 - np.array(out_r))/255.0).astype('float32')\n",
    "#     sift_l = np.array(sift_l).reshape((-1,256))\n",
    "#     sift_r = np.array(sift_r).reshape((-1,256))\n",
    "#     out_y = np.array(out_y).reshape((-1,1))\n",
    "#     return out_l.reshape(-1,imDim,imDim,1), out_r.reshape(-1,imDim,imDim,1),sift_l,sift_r, out_y\n",
    "\n",
    "# def gen_random_val_batch_sift(nb_examples):\n",
    "#     out_l,out_r,sift_l,sift_r,out_y = [],[],[],[],[]\n",
    "#     li = random.sample(range(len(df_val)), nb_examples)\n",
    "#     for i in li:\n",
    "#         out_l.append(ocr.imread(images_folder_val+df_val.iloc[i]['left'],0))\n",
    "#         out_r.append(ocr.imread(images_folder_val+df_val.iloc[i]['right'],0))\n",
    "#         sift_l.append(df_val.iloc[i][3:259])\n",
    "#         sift_r.append(df_val.iloc[i][259:])\n",
    "#         out_y.append(df_val.iloc[i]['label'])\n",
    "#     out_l = ((255.01 - np.array(out_l))/255.0).astype('float32')\n",
    "#     out_r = ((255.01 - np.array(out_r))/255.0).astype('float32')\n",
    "#     sift_l = np.array(sift_l).reshape((-1,256))\n",
    "#     sift_r = np.array(sift_r).reshape((-1,256))\n",
    "#     out_y = np.array(out_y).reshape((-1,1))\n",
    "#     return out_l.reshape(-1,imDim,imDim,1), out_r.reshape(-1,imDim,imDim,1),sift_l,sift_r, out_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "# def generateDatasetAndSave_Siamese():\n",
    "#     label = []\n",
    "#     left_path = []\n",
    "#     right_path = []\n",
    "#     simCounter = 0\n",
    "#     for l,r in combinations(train_files_list,2):\n",
    "#         if(int(l.split('/')[3][:4]) == int(r.split('/')[3][:4])):\n",
    "#             label.append([1])\n",
    "#     #     else:\n",
    "#     #         label.append([0,1])\n",
    "#             left_path.append(l)\n",
    "#             right_path.append(r)\n",
    "#             simCounter+=1\n",
    "#     diffCounter = 0\n",
    "#     for l,r in combinations(train_files_list,2):\n",
    "#         if(int(l.split('/')[3][:4]) != int(r.split('/')[3][:4])):\n",
    "#             label.append([0])\n",
    "#     #     else:\n",
    "#     #         label.append([0,1])\n",
    "#             left_path.append(l)\n",
    "#             right_path.append(r)\n",
    "#             diffCounter+=1\n",
    "#         if(diffCounter==simCounter):\n",
    "#             break\n",
    "#     df = pd.DataFrame()\n",
    "#     pd.options.display.max_colwidth = 100\n",
    "#     df['left'] = left_path\n",
    "#     df['right'] = right_path\n",
    "#     df['label'] = label\n",
    "#     df.to_csv('dataset_training_siamese.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ImageInput (InputLayer)      (None, 250, 250, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 250, 250, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 62, 62, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 31, 31, 256)       33024     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 15, 15, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "=================================================================\n",
      "Total params: 142,144\n",
      "Trainable params: 142,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "set_session(tf.Session(config=config))\n",
    "imDim = 250\n",
    "input_shape  = (imDim,imDim,1)\n",
    "inp_img = Input(shape = (imDim,imDim,1), name = 'ImageInput')\n",
    "model = inp_img\n",
    "\n",
    "#     model = Input(shape=(imDim,imDim,1))\n",
    "#     model.add(Input(shape = (imDim,imDim,1), name = 'FeatureNet_ImageInput'))\n",
    "model = Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same')(model)\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model = MaxPooling2D((2,2), padding='valid')(model)\n",
    "model = Conv2D(64, (3, 3), activation='relu',padding='same')(model)\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))\n",
    "model = MaxPooling2D((2,2),padding='valid')(model)\n",
    "model = Dropout(0.3)(model)\n",
    "#     model.add(Conv2D(16, (3, 3), activation='relu',padding='same'))\n",
    "model = Conv2D(128, (3, 3), activation='relu',padding='same')(model)\n",
    "model = MaxPooling2D((2,2),padding='valid')(model)\n",
    "#     model.add(Conv2D(1, (3, 3), activation='relu',padding='same'))\n",
    "#     model.add(Conv2D(2, (3, 3), activation='relu',padding='same'))\n",
    "model = Dropout(0.3)(model)\n",
    "model = Conv2D(256, (1, 1), activation='relu',padding='valid')(model)\n",
    "model = MaxPooling2D((2,2),padding='valid')(model)\n",
    "model = Dropout(0.3)(model)\n",
    "model = Conv2D(64, (1, 1), activation='relu',padding='valid')(model)\n",
    "model = MaxPooling2D((2,2),padding='valid')(model)\n",
    "\n",
    "model = Flatten()(model)\n",
    "\n",
    "# img_in = np.array((-1,imDim,imDim,1), dtype='float32')\n",
    "# img_in = tf.placeholder(shape=(imDim,imDim,1), dtype='float32')\n",
    "\n",
    "feat = Model(inputs=[inp_img], outputs=[model],name = 'Feat_Model')\n",
    "feat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_img = Input(shape = (imDim,imDim,1), name = 'left_img')\n",
    "right_img = Input(shape = (imDim,imDim,1), name = 'right_img')\n",
    "# left_sift = Input(shape= (256,), name = 'left_sift')\n",
    "# right_sift = Input(shape= (256,), name = 'right_sift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Feat_Model_2/flatten_3/Reshape:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "left_feats = feat(left_img)\n",
    "right_feats = feat(right_img)\n",
    "\n",
    "# inpTensor = Input((1,256,), name = 'SIFTInput')\n",
    "# finalOut = Flatten()(inpTensor)  \n",
    "# sift_model = Model(inputs=[inpTensor], outputs=[finalOut], name='Sift_Model')\n",
    "# sift_model.summary()\n",
    "\n",
    "# left_sift_feats = sift_model(left_sift)\n",
    "# right_sift_feats = sift_model(right_sift)\n",
    "# print(left_sift_feats)\n",
    "print(left_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate #, subtract, multiply, division\n",
    "from keras.utils import to_categorical\n",
    "import random  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "left_img (InputLayer)            (None, 250, 250, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "right_img (InputLayer)           (None, 250, 250, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Feat_Model (Model)               (None, 3136)          142144      left_img[0][0]                   \n",
      "                                                                   right_img[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merged_feats (Concatenate)       (None, 6272)          0           Feat_Model[1][0]                 \n",
      "                                                                   Feat_Model[2][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          6423552     merged_feats[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 1024)          4096        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1024)          0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 1024)          0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4)             4100        dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 4)             16          dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 4)             0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 4)             0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             10          dropout_11[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 6,573,918\n",
      "Trainable params: 6,571,862\n",
      "Non-trainable params: 2,056\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## CNN+SIFT\n",
    "# merged_feats = concatenate([left_feats, right_feats, left_sift_feats, right_sift_feats], name = 'merged_feats')\n",
    "## Only SIFT merge\n",
    "# merged_feats = concatenate([left_sift_feats, right_sift_feats], name = 'merged_feats')\n",
    "## Only CNN merge\n",
    "merged_feats = concatenate([left_feats, right_feats], name = 'merged_feats')\n",
    "merged_feats = Dense(1024, activation = 'linear')(merged_feats)\n",
    "merged_feats = BatchNormalization()(merged_feats)\n",
    "merged_feats = Activation('relu')(merged_feats)\n",
    "merged_feats = Dropout(0.3)(merged_feats)\n",
    "merged_feats = Dense(4, activation = 'linear')(merged_feats)\n",
    "merged_feats = BatchNormalization()(merged_feats)\n",
    "merged_feats = Activation('relu')(merged_feats)\n",
    "merged_feats = Dropout(0.3)(merged_feats)\n",
    "merged_feats = Dense(2, activation = 'softmax')(merged_feats)\n",
    "## only sift\n",
    "# similarity_model = Model(inputs = [left_sift,right_sift], outputs = [merged_feats], name = 'Similarity_Model')\n",
    "## only cnn\n",
    "similarity_model = Model(inputs = [left_img, right_img], outputs = [merged_feats], name = 'Similarity_Model')\n",
    "## with cnn+sift\n",
    "# similarity_model = Model(inputs = [left_img, right_img, left_sift,right_sift], outputs = [merged_feats], name = 'Similarity_Model')\n",
    "similarity_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_folder = ''\n",
    "sgd = K.optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "similarity_model.compile(optimizer=sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "# similarity_model.load_weights('weights/weights-siamese-AND-concat-cnn-0.377613.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = 'pairsDevTrain.csv'\n",
    "val_set = 'pairsDevTest.csv'\n",
    "df_train = pd.DataFrame.from_csv(train_set).reset_index()\n",
    "df_val = pd.DataFrame.from_csv(val_set).reset_index()\n",
    "df_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iteration = 0\n",
    "# left_image = ocr.imread(df.iloc[iteration]['left'],0)\n",
    "# right_image = ocr.imread(df.iloc[iteration]['right'],0)\n",
    "# y = df.iloc[iteration]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on: 118357, Validate on: 919\n",
      "Epoch: 1  Train accuracy: 60.7887%, Validation acc: 49.8368%, Loss: 0.653956 (improved)\n",
      "Epoch: 2  Train accuracy: 68.2613%, Validation acc: 94.4505%, Loss: 0.578641 (improved)\n",
      "Epoch: 3  Train accuracy: 73.0756%, Validation acc: 95.3210%, Loss: 0.535841 (improved)\n",
      "Epoch: 4  Train accuracy: 76.4775%, Validation acc: 96.5180%, Loss: 0.505717 (improved)\n",
      "Epoch: 5  Train accuracy: 79.0383%, Validation acc: 97.3885%, Loss: 0.481943 (improved)\n",
      "Epoch: 6  Train accuracy: 81.0010%, Validation acc: 97.8237%, Loss: 0.462076 (improved)\n",
      "Epoch: 7  Train accuracy: 82.5820%, Validation acc: 98.2590%, Loss: 0.445054 (improved)\n",
      "Epoch: 8  Train accuracy: 83.8905%, Validation acc: 98.3678%, Loss: 0.429687 (improved)\n",
      "Epoch: 9  Train accuracy: 84.9675%, Validation acc: 98.4766%, Loss: 0.415877 (improved)\n",
      "Epoch: 10  Train accuracy: 85.9256%, Validation acc: 98.8030%, Loss: 0.403104 (improved)\n",
      "Epoch: 11  Train accuracy: 86.7376%, Validation acc: 99.2383%, Loss: 0.391431 (improved)\n",
      "Epoch: 12  Train accuracy: 87.4546%, Validation acc: 99.5647%, Loss: 0.380556 (improved)\n",
      "Epoch: 13  Train accuracy: 88.0985%, Validation acc: 99.7824%, Loss: 0.370387 (improved)\n",
      "Epoch: 14  Train accuracy: 88.6766%, Validation acc: 99.5647%, Loss: 0.360815 (improved)\n",
      "Epoch: 15  Train accuracy: 89.1903%, Validation acc: 99.1295%, Loss: 0.351979 (improved)\n",
      "Epoch: 16  Train accuracy: 89.6534%, Validation acc: 96.3003%, Loss: 0.343753 (improved)\n",
      "Epoch: 17  Train accuracy: 90.0776%, Validation acc: 94.2329%, Loss: 0.335935 (improved)\n",
      "Epoch: 18  Train accuracy: 90.4632%, Validation acc: 90.8596%, Loss: 0.328512 (improved)\n",
      "Epoch: 19  Train accuracy: 90.8177%, Validation acc: 86.2894%, Loss: 0.321524 (improved)\n",
      "Training the model: 7/231 (3.0%)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-99da44f20f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files_list\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mleft_image\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mright_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msift_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msift_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_random_train_batch_sift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#         print('\\n------\\n'+str(y)+'\\n------\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3e56a80f0a31>\u001b[0m in \u001b[0;36mgen_random_train_batch_sift\u001b[1;34m(nb_examples)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mout_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_folder_tr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mout_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mocr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_folder_tr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'right'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0msift_l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m259\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "imgDim =  250\n",
    "best_loss_tr = np.infty\n",
    "train_files_list = df_train.shape[0]\n",
    "train_num_examples = train_files_list\n",
    "print('Train on: %2d, Validate on: %2d' %(train_num_examples, df_val.shape[0]))\n",
    "n_iterations_per_epoch = train_num_examples // batch_size\n",
    "# n_iterations_validation = val_num_examples // batch_size\n",
    "# Set's how much noise we're adding to the MNIST images\n",
    "noise_factor = 0.0\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tr_loss_vals = []\n",
    "tr_acc_vals = []\n",
    "val_acc_vals = []\n",
    "v_batch_size = df_val.shape[0]\n",
    "early_stopping_counter = 0\n",
    "patience = 5\n",
    "max_val_acc = 0\n",
    "\n",
    "batch_metrics = None\n",
    "batch_metric_loss = []\n",
    "batch_metric_acc = []\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(int(train_files_list/batch_size)):        \n",
    "        left_image ,right_imagey = gen_random_train_batch(batch_size)\n",
    "        y = to_categorical(y)\n",
    "#         print('\\n------\\n'+str(y)+'\\n------\\n')\n",
    "        left_image = left_image.reshape((-1, imgDim, imgDim, 1))\n",
    "        right_image = right_image.reshape((-1, imgDim, imgDim, 1))\n",
    "#         batch_metrics = similarity_model.train_on_batch([left_image,right_image,sift_l,sift_r],[y])\n",
    "#         batch_metrics = similarity_model.train_on_batch([sift_l,sift_r],[y])\n",
    "        batch_metrics = similarity_model.train_on_batch([left_image,right_image],[y])\n",
    "        \n",
    "        batch_metric_loss.append(batch_metrics[0])\n",
    "        batch_metric_acc.append(batch_metrics[1])\n",
    "        print(\"\\rTraining the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_per_epoch,\n",
    "                  iteration * 100 / n_iterations_per_epoch),\n",
    "              end=\"\" * 10)\n",
    "    loss_tr = np.mean(batch_metric_loss)\n",
    "    acc_tr = np.mean(batch_metric_acc)\n",
    "    tr_loss_vals.append(loss_tr)\n",
    "    tr_acc_vals.append(acc_tr)\n",
    "    \n",
    "    v_left_image ,v_right_image, v_y = gen_random_val_batch(v_batch_size)\n",
    "    \n",
    "#     pred_sim = similarity_model.predict([v_left_image ,v_right_image,v_sift_l, v_sift_r])\n",
    "#     pred_sim = similarity_model.predict([v_sift_l, v_sift_r])\n",
    "    pred_sim = similarity_model.predict([v_left_image ,v_right_image])\n",
    "    pred_val_sim = []\n",
    "    for p in pred_sim:\n",
    "        pred_val_sim.append(np.argmax(p))\n",
    "    v_acc = float((np.array(pred_val_sim).reshape((v_batch_size,1))==v_y).sum())/float(len(v_y))\n",
    "    val_acc_vals.append(v_acc)\n",
    "    \n",
    "    print(\"\\rEpoch: {}  Train accuracy: {:.4f}%, Validation acc: {:.4f}%, Loss: {:.6f}{}\".format(\n",
    "        epoch + 1, (acc_tr) * 100, (v_acc)*100.0, loss_tr,\n",
    "        \" (improved)\" if loss_tr < best_loss_tr else \"\"))\n",
    "    if loss_tr < best_loss_tr and ((epoch % 1)==0):\n",
    "#             save_path = saver.save(sess, checkpoint_path)\n",
    "        best_loss_tr = loss_tr\n",
    "        filepath='weights/siam-cnn-concat-[val-acc-\"+str(v_acc)+\"].hdf5\"\n",
    "        similarity_model.save(filepath)\n",
    "    if max_val_acc < v_acc:\n",
    "        max_val_acc = v_acc\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter+=1\n",
    "    if early_stopping_counter==patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame()\n",
    "df_metrics['accuracy'] = tr_acc_vals\n",
    "df_metrics['loss'] = tr_loss_vals\n",
    "df_metrics['val_acc'] = val_acc_vals\n",
    "df_metrics.to_csv('plots/siam-cnn-concat-metric-cnn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9Eop1cv9fwEWI6nrPbjGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24751ad99e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ax = df_metrics.plot()\n",
    "fig = ax.figure\n",
    "fig.savefig('plots/siam-cnn-concat-plot-cnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value conv2d_11/kernel\n\t [[Node: conv2d_11/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv2d_11/kernel\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv2d_11/kernel)]]\n\t [[Node: dense_3/Softmax/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_549_dense_3/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_11/kernel/read', defined at:\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-d906c5a0b103>\", line 12, in <module>\n    model = Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same')(model)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\topology.py\", line 569, in __call__\n    self.build(input_shapes[0])\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 320, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1303, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv2d_11/kernel\n\t [[Node: conv2d_11/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv2d_11/kernel\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv2d_11/kernel)]]\n\t [[Node: dense_3/Softmax/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_549_dense_3/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv2d_11/kernel\n\t [[Node: conv2d_11/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv2d_11/kernel\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv2d_11/kernel)]]\n\t [[Node: dense_3/Softmax/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_549_dense_3/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0bc4329c65bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# a completely untrained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshow_model_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-0bc4329c65bd>\u001b[0m in \u001b[0;36mshow_model_output\u001b[1;34m(nb_examples)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpv_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpv_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpv_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_random_val_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     pred_sim = similarity_model.predict([pv_a, pv_b,pv_s_l,pv_s_r])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpred_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpv_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpv_b\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mp_val_argmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1592\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1594\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1216\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv2d_11/kernel\n\t [[Node: conv2d_11/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv2d_11/kernel\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv2d_11/kernel)]]\n\t [[Node: dense_3/Softmax/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_549_dense_3/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_11/kernel/read', defined at:\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-d906c5a0b103>\", line 12, in <module>\n    model = Conv2D(32,kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same')(model)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\topology.py\", line 569, in __call__\n    self.build(input_shapes[0])\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 134, in build\n    constraint=self.kernel_constraint)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 320, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 200, in __init__\n    expected_shape=expected_shape)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1303, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Jyoti Kini\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv2d_11/kernel\n\t [[Node: conv2d_11/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv2d_11/kernel\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv2d_11/kernel)]]\n\t [[Node: dense_3/Softmax/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_549_dense_3/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# similarity_model = load model (best one)\n",
    "%matplotlib inline\n",
    "def show_model_output(nb_examples = 5):\n",
    "    \n",
    "#     pv_a, pv_b,pv_s_l,pv_s_r, pv_sim = gen_random_val_batch(nb_examples)\n",
    "    pv_a, pv_b, pv_sim = gen_random_val_batch(nb_examples)\n",
    "#     pred_sim = similarity_model.predict([pv_a, pv_b,pv_s_l,pv_s_r])\n",
    "    pred_sim = similarity_model.predict([pv_a, pv_b])\n",
    "    p_val_argmax=[]\n",
    "    print(pred_sim)\n",
    "    for p in pred_sim:\n",
    "        p_val_argmax.append(np.argmax(p))\n",
    "    p_val_argmax = np.array(p_val_argmax)\n",
    "    fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n",
    "    for c_a, c_b, c_d, p_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, pred_sim, m_axs.T):\n",
    "        ax1.imshow(c_a[:,:,0])\n",
    "        ax1.set_title('Left Image\\n Actual Label: %3.0f%%' % (100*c_d))\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(c_b[:,:,0])\n",
    "        ax2.set_title('Right Image\\n Different %3.0f%% \\nSame %3.0f%% ' % (100*p_d[0],100*p_d[1]))\n",
    "        ax2.axis('off')\n",
    "    return fig\n",
    "# a completely untrained model\n",
    "_ = show_model_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_left_image ,v_right_image, v_y = gen_random_val_batch(100)\n",
    "pred_sim = similarity_model.predict([v_left_image ,v_right_image])\n",
    "(pred_sim.round().astype('int')==v_y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l,r,y = gen_random_train_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(range(1, 10), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(feature_model, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(similarity_model, show_layer_names=True, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,1,1,1,1,1,1,1,1,0])\n",
    "b = np.array([1,1,1,1,1,1,1,1,0,0])\n",
    "(a==b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l,r,s,y = gen_random_val_batch_sift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow-gpu]",
   "language": "python",
   "name": "Python [tensorflow-gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
